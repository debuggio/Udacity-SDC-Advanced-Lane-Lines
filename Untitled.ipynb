{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'camera_cal/calibration_pickle.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-101f55603ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Read in the saved objpoints and imgpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdist_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m\"camera_cal/calibration_pickle.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_pickle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mtx\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_pickle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dist\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'camera_cal/calibration_pickle.p'"
     ]
    }
   ],
   "source": [
    "# Created By Aaron Brown\n",
    "# January 20, 2017\n",
    "# Udacity Self-Driving Car Nanodegree\n",
    "\n",
    "# Main class to generate video output\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from LineTracker import Tracker\n",
    "\n",
    "# Read in the saved objpoints and imgpoints\n",
    "dist_pickle = pickle.load( open( \"camera_cal/calibration_pickle.p\", \"rb\" ) )\n",
    "mtx = dist_pickle[\"mtx\"]\n",
    "dist = dist_pickle[\"dist\"]\n",
    "\n",
    "# (Selected_Video) Preset Video parameters (there are 3 videos, project_video: 1, challenge_video:2, and harder_challenge_video:3)\n",
    "# WHICH VIDEO DO YOU WANT TO WORK ON?\n",
    "Selected_Video = 3\n",
    "\n",
    "if Selected_Video == 1:\n",
    "    # set up thresholding for the appropriate video to get best pixels of interest\n",
    "    # For these videos we are focusing on using x/y gradients and hsv and hls color spaces, which seemed most useful for finding important pixels\n",
    "    gradx_thresh = (25,255) # gradient x threshold\n",
    "    grady_thresh = (10,255) # gradient y threshold\n",
    "    schannel_thresh = (100,255) # gradient s channel threshold\n",
    "    vchannel_thresh = (200,255) # gradient v channel threshold\n",
    "\n",
    "    # preset percentages to transform road image, want road lines to be as parallel as possible for LineTracker\n",
    "    bot_width = .76 # percent of bottom trapizoid height #.76\n",
    "    mid_width = .1 # percent of middle trapizoid height .1 # .3\n",
    "    height_pct = .63 # percent for trapizoid height .63\n",
    "    bottom_trim = .935 # percent from top to bottom to avoid car hood \n",
    "\n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = Tracker(Mycenter_dis = .25*1280, Mywindow_width = 25, Mywindow_height = 40, Mypadding = 25, Myslide_res = 5, Myframe_ps = 25, Mycapture_height = 720, \\\n",
    "                            My_ym = 30/720, My_xm = 4/384, Myline_dist = 10)\n",
    "\n",
    "    # set output and input video names\n",
    "    Output_video = 'output1_tracked.mp4'\n",
    "    Input_video = 'project_video.mp4'\n",
    "\n",
    "elif Selected_Video == 2:\n",
    "    # set up thresholding for the appropriate video to get best pixels of interest\n",
    "    # For these videos we are focusing on using x/y gradients and hsv and hls color spaces, which seemed most useful for finding important pixels\n",
    "    gradx_thresh = (20,255) # gradient x threshold\n",
    "    grady_thresh = (20,255) # gradient y threshold\n",
    "    schannel_thresh = (8,255) # gradient s channel threshold\n",
    "    vchannel_thresh = (160,255) # gradient v channel threshold\n",
    "\n",
    "    # preset percentages to transform road image, want road lines to be as parallel as possible for LineTracker\n",
    "    bot_width = .76 # percent of bottom trapizoid height #.76\n",
    "    mid_width = .25 # percent of middle trapizoid height .1 # .3\n",
    "    height_pct = .7 # percent for trapizoid height .63\n",
    "    bottom_trim = .935 # percent from top to bottom to avoid car hood \n",
    "\n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = Tracker(Mycenter_dis = .25*1280, Mywindow_width = 25, Mywindow_height = 40, Mypadding = 25, Myslide_res = 5, Myframe_ps = 25, Mycapture_height = 720, \\\n",
    "                            My_ym = 15/720, My_xm = 4/384, Myline_dist = 3)\n",
    "\n",
    "    # set output and input video names\n",
    "    Output_video = 'output2_tracked.mp4'\n",
    "    Input_video = 'challenge_video.mp4'\n",
    "\n",
    "elif Selected_Video == 3:\n",
    "\n",
    "    # set up thresholding for the appropriate video to get best pixels of interest\n",
    "    # For these videos we are focusing on using x/y gradients and hsv and hls color spaces, which seemed most useful for finding important pixels\n",
    "    gradx_thresh = (20,255) # gradient x threshold\n",
    "    grady_thresh = (20,255) # gradient y threshold\n",
    "    schannel_thresh = (240,255) # gradient s channel threshold\n",
    "    vchannel_thresh = (240,255) # gradient v channel threshold\n",
    "\n",
    "    # preset percentages to transform road image, want road lines to be as parallel as possible for LineTracker\n",
    "    bot_width = .76 # percent of bottom trapizoid height #.76\n",
    "    mid_width = .3 # percent of middle trapizoid height .1 # .3\n",
    "    height_pct = .73 # percent for trapizoid height .63\n",
    "    bottom_trim = .935 # percent from top to bottom to avoid car hood \n",
    "\n",
    "    # Set up the overall class to do all the tracking\n",
    "    curve_centers = Tracker(Mycenter_dis = .25*1280, Mywindow_width = 25, Mywindow_height = 40, Mypadding = 25, Myslide_res = 5, Myframe_ps = 25, Mycapture_height = 720, \\\n",
    "                            My_ym = 10/720, My_xm = 4/384, Myline_dist = 3)\n",
    "\n",
    "    # set output and input video names\n",
    "    Output_video = 'output3_tracked.mp4'\n",
    "    Input_video = 'harder_challenge_video.mp4'\n",
    "\n",
    "\n",
    "# Useful functions for producing the binary pixel of interest images to feed into the LaneTracker Algorithm\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    # Apply threshold\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx**2 + sobely**2)\n",
    "    scale_factor = np.max(gradmag)/255 \n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    binary_output = np.zeros_like(gradmag)\n",
    "    # Apply threshold\n",
    "    binary_output[(gradmag >= mag_thresh[0]) & (gradmag <= mag_thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        absgraddir = np.absolute(np.arctan(sobely/sobelx))\n",
    "        binary_output =  np.zeros_like(absgraddir)\n",
    "        # Apply threshold\n",
    "        binary_output[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def color_threshold(image, sthresh=(0,255), vthresh=(0,255)):\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    s_channel = hls[:,:,2]\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= sthresh[0]) & (s_channel <= sthresh[1])  ] = 1\n",
    "\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v_channel = hsv[:,:,2]\n",
    "    v_binary = np.zeros_like(v_channel)\n",
    "    v_binary[(v_channel >= vthresh[0]) & (v_channel <= vthresh[1])  ] = 1\n",
    "\n",
    "    output = np.zeros_like(s_channel)\n",
    "    output[(s_binary == 1) & (v_binary == 1)] = 1\n",
    "    return output\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    if(((center-width/2) < img_ref.shape[1]) & ((center+width/2) > 0)):\n",
    "        output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width)):min(img_ref.shape[1],int(center+width))] = 1\n",
    "    return output\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)   \n",
    "\n",
    "def process_image(image):\n",
    "\n",
    "    # undistort the image \n",
    "    img = cv2.undistort(image,mtx,dist,None,mtx)\n",
    "\n",
    "    # process image and generate binary pixel of interests\n",
    "    preprocessImage = np.zeros_like(img[:,:,0])\n",
    "    gradx = abs_sobel_thresh(img, orient='x', thresh=gradx_thresh) \n",
    "    grady = abs_sobel_thresh(img, orient='y', thresh=grady_thresh) \n",
    "    c_binary = color_threshold(img, sthresh=schannel_thresh, vthresh=vchannel_thresh)\n",
    "    preprocessImage[((gradx == 1) & (grady == 1) | (c_binary == 1) )] = 255\n",
    "\n",
    "    # work on defining perspective transformation area\n",
    "    img_size = (img.shape[1],img.shape[0])\n",
    "    src = np.float32([[img.shape[1]*(.5-mid_width/2),img.shape[0]*height_pct],[img.shape[1]*(.5+mid_width/2),img.shape[0]*height_pct],[img.shape[1]*(.5+bot_width/2),img.shape[0]*bottom_trim],[img.shape[1]*(.5-bot_width/2),img.shape[0]*bottom_trim]])\n",
    "    offset = img_size[0]*.33\n",
    "    dst = np.float32([[offset, 0], [img_size[0]-offset, 0],[img_size[0]-offset, img_size[1]], [offset ,img_size[1]]])\n",
    "\n",
    "    # perform the transform\n",
    "    M = cv2.getPerspectiveTransform(src,dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst,src)\n",
    "    warped = cv2.warpPerspective(preprocessImage,M,img_size,flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # for debugging just to get the binary image\n",
    "    #warped = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    #return warped\n",
    "\n",
    "    # set up the analysis for speed measurement\n",
    "    spd_offset = img_size[0]*(1.0-bot_width)/2\n",
    "    spd_dst = np.float32([[spd_offset, 0], [img_size[0]-spd_offset, 0],[img_size[0]-spd_offset, img_size[1]], [spd_offset ,img_size[1]]])\n",
    "    spd_M = cv2.getPerspectiveTransform(src,spd_dst)\n",
    "    # generate gray scale image to use for tracking speed in LineTracker, using template matching\n",
    "    warped_mono = cv2.warpPerspective(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY),spd_M,img_size,flags=cv2.INTER_LINEAR)\n",
    "    curve_centers.speed_track(warped_mono)\n",
    "\n",
    "    # find the best line centers based on the binary pixel of interest input\n",
    "    frame_centers = curve_centers.track_line(warped)\n",
    "    # need these parameters to draw the graphic overlay illustraing the window convolution matching\n",
    "    window_width = curve_centers.window_width \n",
    "    window_height = curve_centers.window_height\n",
    "    # points used for graphic overlay \n",
    "    l_points = np.zeros_like(warped)\n",
    "    r_points = np.zeros_like(warped)\n",
    "\n",
    "    # points used to find the left and right lanes\n",
    "    rightx = []\n",
    "    leftx = []\n",
    "\n",
    "    res_yvals = np.arange(warped.shape[0]-(window_height+window_height/2),0,-window_height)\n",
    "\n",
    "    for level in range(1,len(frame_centers)):\n",
    "        l_mask = window_mask(window_width,window_height,warped,frame_centers[level][0],level)\n",
    "        r_mask = window_mask(window_width,window_height,warped,frame_centers[level][1],level)\n",
    "        # add center value found in frame to the list of lane points per left,right\n",
    "        leftx.append(frame_centers[level][0])\n",
    "        rightx.append(frame_centers[level][1])\n",
    "        # fill in graphic points here if pixels fit inside the specificed window from l/r mask\n",
    "        l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "        r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "    # drawing the graphic overlay to represents the results found for tracking window centers\n",
    "    template = np.array(r_points+l_points,np.uint8)\n",
    "    zero_channel = np.zeros_like(template)\n",
    "    template = np.array(cv2.merge((zero_channel,zero_channel,template)),np.uint8)\n",
    "    warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "    graphic_measure = cv2.addWeighted(warpage, 0.2, template, 0.75, 0.0)\n",
    "\n",
    "\n",
    "    # fit the lane boundaries to the left,right center positions found\n",
    "    yvals = range(0,warped.shape[0])\n",
    "\n",
    "    left_fit = np.polyfit(res_yvals, leftx, 3)\n",
    "    left_fitx = left_fit[0]*yvals*yvals*yvals + left_fit[1]*yvals*yvals + left_fit[2]*yvals+left_fit[3]\n",
    "    left_fitx = np.array(left_fitx,np.int32)\n",
    "\n",
    "    right_fit = np.polyfit(res_yvals, rightx, 3)\n",
    "    right_fitx = right_fit[0]*yvals*yvals*yvals + right_fit[1]*yvals*yvals + right_fit[2]*yvals+right_fit[3]\n",
    "    right_fitx = np.array(right_fitx,np.int32)\n",
    "\n",
    "    # used to find center curve\n",
    "    curve_xpts = [(right_fitx[0]+left_fitx[0])/2,(right_fitx[len(right_fitx)/2]+left_fitx[len(left_fitx)/2])/2,(right_fitx[-1]+left_fitx[-1])/2]\n",
    "    curve_ypts = [yvals[0],yvals[(int)(len(yvals)/2)],yvals[-1]]\n",
    "    curve_fit = np.polyfit(curve_ypts, curve_xpts, 2)\n",
    "    curve_fitx = curve_fit[0]*yvals*yvals + curve_fit[1]*yvals+ curve_fit[2]\n",
    "\n",
    "    # used to format everything so its ready for cv2 draw functions\n",
    "    left_lane = np.array(list(zip(np.concatenate((left_fitx-window_width/2,left_fitx[::-1]+window_width/2), axis=0),np.concatenate((yvals,yvals[::-1]),axis=0))),np.int32)\n",
    "    right_lane = np.array(list(zip(np.concatenate((right_fitx-window_width/2,right_fitx[::-1]+window_width/2), axis=0),np.concatenate((yvals,yvals[::-1]),axis=0))),np.int32)\n",
    "    inner_lane = np.array(list(zip(np.concatenate((left_fitx+window_width/2,right_fitx[::-1]-window_width/2), axis=0),np.concatenate((yvals,yvals[::-1]),axis=0))),np.int32)\n",
    "    curve_pts = np.array([list(zip(curve_fitx,yvals))],np.int32)\n",
    "\n",
    "    # draw lane lines, middle curve, road background on two different blank overlays\n",
    "    road = np.zeros_like(template)\n",
    "    road_bkg = np.zeros_like(template)\n",
    "    cv2.fillPoly(road,[left_lane],color=[9, 67, 109])\n",
    "    cv2.fillPoly(road,[right_lane],color=[9, 67, 109])\n",
    "    cv2.polylines(road,[curve_pts],isClosed=False, color=[5, 176, 249], thickness=3)\n",
    "    for horz_line_y in curve_centers.horz_lines:\n",
    "        cv2.line(road,(left_fitx[(int)(horz_line_y)],(int)(horz_line_y)),(right_fitx[(int)(horz_line_y)],(int)(horz_line_y)),color=[5, 176, 249], thickness=3)\n",
    "\n",
    "    cv2.fillPoly(road_bkg,[inner_lane],color=[38, 133, 197])\n",
    "\n",
    "    # after done drawing all the marking effects, warp back image to its orginal perspective.\n",
    "    # Note for the two different overlays, just seperating road_warped and road_warped_bkg to get two different alpha values, its just for astetics...\n",
    "    road_warped = cv2.warpPerspective(road,Minv,img_size,flags=cv2.INTER_LINEAR)\n",
    "    road_warped_bkg = cv2.warpPerspective(road_bkg,Minv,img_size,flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    # merging all the different overlays, basically make things look pretty!\n",
    "    lane_template = np.array(cv2.merge((road_warped[:,:,2],road_warped[:,:,2],road_warped[:,:,2])),np.uint8)\n",
    "    bkg_template = np.array(cv2.merge((road_warped_bkg[:,:,2],road_warped_bkg[:,:,2],road_warped_bkg[:,:,2])),np.uint8)\n",
    "    base = cv2.addWeighted(img, 1.0, bkg_template, -0.6, 0.0)\n",
    "    base = cv2.addWeighted(base, 1.0, road_warped_bkg, 0.6, 0.0)\n",
    "    base = cv2.addWeighted(base, 1.0, lane_template, -1.8, 0.0)\n",
    "    result = cv2.addWeighted(base, 1.0, road_warped, 0.9, 0.0)\n",
    "    #return result\n",
    "\n",
    "    # calcuate the middle line curvature\n",
    "    ym_per_pix = curve_centers.ym_per_pix # meters per pixel in y dimension\n",
    "    xm_per_pix = curve_centers.xm_per_pix # meteres per pixel in x dimension\n",
    "    curve_fit_cr = np.polyfit(np.array(curve_ypts,np.float32)*ym_per_pix, np.array(curve_xpts,np.float32)*xm_per_pix, 2)\n",
    "    curverad = ((1 + (2*curve_fit_cr[0]*curve_ypts[1]*ym_per_pix + curve_fit_cr[1])**2)**1.5) /np.absolute(2*curve_fit_cr[0])\n",
    "    curve_centers.curvatures.append(curverad)\n",
    "    curverad = curve_centers.SmoothCurve()\n",
    "\n",
    "    # calculate the speed of the car\n",
    "    speed = curve_centers.CalculateSpeed(metric_mode = False)\n",
    "\n",
    "    # calculate the offset of the car on the road\n",
    "    camera_center = (left_fitx[-1] + right_fitx[-1])/2\n",
    "    center_diff = (camera_center-warped.shape[1]/2)*xm_per_pix\n",
    "    side_pos = 'left'\n",
    "    if center_diff <= 0:\n",
    "        side_pos = 'right'\n",
    "\n",
    "    # add text backdrop\n",
    "    txt_bkg = np.zeros_like(result)\n",
    "    txt_bkg_pts = np.array([(0,0),((int)(result.shape[1]*.5),0),((int)(result.shape[1]*.5),(int)(result.shape[0]*.25)),(0,(int)(result.shape[0]*.25))])\n",
    "    cv2.fillPoly(txt_bkg,[txt_bkg_pts],color=[200, 200, 200])\n",
    "    result = cv2.addWeighted(result, 1.0, txt_bkg, -1.0, 0.0)\n",
    "    # draw the text showing curvature, offset, and speed\n",
    "    cv2.putText(result,'Radius of Curvature = '+str(round(curverad,3))+'(m)',(50,50) , cv2.FONT_HERSHEY_SIMPLEX, 1,(5, 176, 249),2)\n",
    "    cv2.putText(result,'Vehicle is '+str(abs(round(center_diff,3)))+'m '+side_pos+' of center',(50,100) , cv2.FONT_HERSHEY_SIMPLEX, 1,(5, 176, 249),2)\n",
    "    cv2.putText(result,'Estimated Speed is '+str(abs(round(speed,3)))+' MPH',(50,150) , cv2.FONT_HERSHEY_SIMPLEX, 1,(5, 176, 249),2)\n",
    "\n",
    "    # insert graphic overlay map\n",
    "    graphic_bkg = np.zeros_like(result)\n",
    "    # scale the graphic measure that we generated near the start for finding window centers, by some constant factor in both axis\n",
    "    g_scale = 0.4\n",
    "    graphic_overlay = cv2.resize(graphic_measure, (0,0), fx=g_scale, fy=g_scale) \n",
    "    g_xoffset = result.shape[1]-graphic_overlay.shape[1]\n",
    "    # overlay the graphic measure in the result image at the top right corner\n",
    "    result[:graphic_overlay.shape[0], g_xoffset:g_xoffset+graphic_overlay.shape[1]] = graphic_overlay\n",
    "\n",
    "    return result\n",
    "\n",
    "video_output = Output_video\n",
    "clip1 = VideoFileClip(Input_video)\n",
    "video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "video_clip.write_videofile(video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
